I work on analytical problems where results are expected to be questioned, reused, and acted on.

Much of analysis fails quietly: numbers appear reasonable, outputs look clean, and decisions proceed anyway. My focus is on identifying when results are not yet safe to use — where assumptions are fragile, inputs dominate outcomes, or apparent signals collapse under scrutiny.

I am deliberate about how conclusions are formed. I test sensitivity, compare approaches, and slow interpretation when evidence is thin. When uncertainty remains, I make it explicit rather than smoothing it away.

The work collected here reflects that approach. These projects prioritise structure, comparison, and failure modes over novelty or volume. They are not demonstrations of tools, but demonstrations of judgement — how conclusions are earned, constrained, and defended once they begin to influence real decisions.
